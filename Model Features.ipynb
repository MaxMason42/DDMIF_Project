{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the papers:\n",
    "\n",
    "@article{wood2021trading,\n",
    "  title={Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture},\n",
    "  author={Wood, Kieran and Giegerich, Sven and Roberts, Stephen and Zohren, Stefan},\n",
    "  journal={arXiv preprint arXiv:2112.08534},\n",
    "  year={2021}\n",
    "}\n",
    "\n",
    "@article {Wood111,\n",
    "\tauthor = {Wood, Kieran and Roberts, Stephen and Zohren, Stefan},\n",
    "\ttitle = {Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection},\n",
    "\tvolume = {4},\n",
    "\tnumber = {1},\n",
    "\tpages = {111--129},\n",
    "\tyear = {2022},\n",
    "\tdoi = {10.3905/jfds.2021.1.081},\n",
    "\tpublisher = {Institutional Investor Journals Umbrella},\n",
    "\tissn = {2640-3943},\n",
    "\tURL = {https://jfds.pm-research.com/content/4/1/111},\n",
    "\teprint = {https://jfds.pm-research.com/content/4/1/111.full.pdf},\n",
    "\tjournal = {The Journal of Financial Data Science}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install empyrical-reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empyrical import (sharpe_ratio, max_drawdown, downside_risk, annual_return, annual_volatility,)\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_performance_metrics(data: pd.DataFrame, metric_suffix=\"\", num_identifiers = None):\n",
    "    if not num_identifiers:\n",
    "        num_identifiers = len(data.dropna()[\"ticker\"].unique()) #Might just make ticker or permno\n",
    "    \n",
    "    srs = data.dropna().groupby(level=0)[\"captured_returns\"].sum() / num_identifiers\n",
    "    \n",
    "    return {\n",
    "        f\"annual_return{metric_suffix}\": annual_return(srs),\n",
    "        f\"annual_volatility{metric_suffix}\": annual_volatility(srs),\n",
    "        f\"sharpe_ratio{metric_suffix}\": sharpe_ratio(srs),\n",
    "        f\"downside_risk{metric_suffix}\": downside_risk(srs),\n",
    "        f\"max_drawdown{metric_suffix}\": -max_drawdown(srs),\n",
    "        f\"perc_pos_return{metric_suffix}\": len(srs[srs > 0.0]) / len(srs),\n",
    "        f\"profit_loss_ratio{metric_suffix}\": np.mean(srs[srs > 0.0])/ np.mean(np.abs(srs[srs < 0.0]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function calc_net_returns used to get volitility adjusted returns which I don't know if we care about\n",
    "#Also used to get the captured returns minus the transaction costs which might be useful\n",
    "#Not going to implement now but in the classical_strategies file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sharpe_by_year(data: pd.DataFrame, suffix=\"\"):\n",
    "    data = data.copy()\n",
    "    data[\"year\"] = data.index.year\n",
    "\n",
    "    sharpes = (\n",
    "        data.dropna()[[\"year\", \"captured_returns\"]]\n",
    "        .groupby(level=0)\n",
    "        .mean()\n",
    "        .groupby(\"year\")\n",
    "        .apply(lambda y: sharpe_ratio(y[\"captured_returns\"]))\n",
    "    )\n",
    "\n",
    "    sharpes.index = \"sharpe_ratio_\" + sharpes.index.map(int).map(str) + suffix\n",
    "\n",
    "    return sharpes.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_returns(srs: pd.Series, day_offset: int = 1):\n",
    "    returns = srs / srs.shift(day_offset) - 1\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_daily_vol(daily_returns):\n",
    "    return (\n",
    "        daily_returns.ewm(span = 60, min_periods = 60).std().fillna(method=\"bfill\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vol_scaled_returns(daily_returns, daily_vol=pd.Series(None)):\n",
    "    if not len(daily_vol):\n",
    "        daily_vol = calc_daily_vol(daily_returns)\n",
    "    annualized_vol = daily_vol * np.sqrt(252)\n",
    "    return daily_returns / annualized_vol.shift(1) #Had multiplication by target vol but don't care about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MACDStrat:\n",
    "    def __init__(self, trend_combinations: List[Tuple[float, float]] = None):\n",
    "        if trend_combinations is None:\n",
    "            self.trend_combinations = [(8, 24), (16, 48), (32, 96)]\n",
    "        else:\n",
    "            self.trend_combinations = trend_combinations \n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_signal(prices: pd.Series, short_timescale: int, long_timescale: int):\n",
    "\n",
    "        def calc_halflife(timescale):\n",
    "            return np.log(0.5) / np.log(1 - 1/timescale)\n",
    "        \n",
    "        macd = (\n",
    "            prices.ewm(halflife= calc_halflife(short_timescale)).mean() - prices.ewm(halflife = calc_halflife(long_timescale)).mean()\n",
    "        )\n",
    "\n",
    "        q = macd / prices.rolling(63).std().fillna(method=\"bfill\") #Standardize MACD with volatility \n",
    "        return q / q.rolling(252).std().fillna(method=\"bfill\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_changepoint_file(file_path: str, lookback_window_length: int):\n",
    "    return (\n",
    "        pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "        .fillna(method=\"ffill\")\n",
    "        .dropna() \n",
    "        .assign(\n",
    "            cp_location_norm=lambda row: (row[\"t\"] - row[\"cp_location\"])/ lookback_window_length\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cpd_features(folder_path: str, lookback_window_length: int):\n",
    "    return pd.concat(\n",
    "        [\n",
    "            read_changepoint_file(\n",
    "                os.path.join(folder_path, f), lookback_window_length\n",
    "            ).assign(ticker=os.path.splitext(f)[0])\n",
    "            for f in os.listdir(folder_path)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_momentum_features(df_asset:pd.DataFrame):\n",
    "    df_asset[\"srs\"] = df_asset[\"close\"]\n",
    "    ewm = df_asset[\"srs\"].ewm(halflife=252)\n",
    "    means = ewm.mean()\n",
    "    stds = ewm.std()\n",
    "    df_asset[\"srs\"] = np.minimum(df_asset[\"srs\"], means + 5 * stds)\n",
    "    df_asset[\"srs\"] = np.maximum(df_asset[\"srs\"], means - 5 * stds)\n",
    "\n",
    "    df_asset[\"daily_returns\"] = calc_returns(df_asset[\"srs\"])\n",
    "    df_asset[\"daily_vol\"] = calc_daily_vol(df_asset[\"daily_returns\"])\n",
    "\n",
    "    df_asset[\"target_returns\"] = calc_vol_scaled_returns(\n",
    "        df_asset[\"daily_returns\"], df_asset[\"daily_vol\"]\n",
    "    ).shift(-1)\n",
    "\n",
    "    def calc_normalized_returns(day_offset):\n",
    "        return (\n",
    "            calc_returns(df_asset[\"srs\"], day_offset) / df_asset[\"daily_vol\"] / np.sqrt(day_offset)\n",
    "        )\n",
    "\n",
    "    df_asset[\"norm_daily_return\"] = calc_normalized_returns(1)\n",
    "    df_asset[\"norm_monthly_return\"] = calc_normalized_returns(21)\n",
    "    df_asset[\"norm_quarterly_return\"] = calc_normalized_returns(63)\n",
    "    df_asset[\"norm_biannual_return\"] = calc_normalized_returns(126)\n",
    "    df_asset[\"norm_annual_return\"] = calc_normalized_returns(252)\n",
    "\n",
    "    trend_combinations = [(8, 24), (16, 48), (32, 96)]\n",
    "    for short_window, long_window in trend_combinations:\n",
    "        df_asset[f\"macd_{short_window}_{long_window}\"] = MACDStrat.calc_signal(\n",
    "            df_asset[\"srs\"], short_window, long_window\n",
    "        )\n",
    "\n",
    "    # date features\n",
    "    if len(df_asset):\n",
    "        df_asset[\"day_of_week\"] = df_asset.index.dayofweek\n",
    "        df_asset[\"day_of_month\"] = df_asset.index.day\n",
    "        df_asset[\"week_of_year\"] = df_asset.index.isocalendar().week\n",
    "        df_asset[\"month_of_year\"] = df_asset.index.month\n",
    "        df_asset[\"year\"] = df_asset.index.year\n",
    "        df_asset[\"date\"] = df_asset.index \n",
    "    else:\n",
    "        df_asset[\"day_of_week\"] = []\n",
    "        df_asset[\"day_of_month\"] = []\n",
    "        df_asset[\"week_of_year\"] = []\n",
    "        df_asset[\"month_of_year\"] = []\n",
    "        df_asset[\"year\"] = []\n",
    "        df_asset[\"date\"] = []\n",
    "    \n",
    "    return df_asset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_changepoint_features(features: pd.DataFrame, cpd_folder_name: str, lookback_window_length: int):\n",
    "    features = features.merge(\n",
    "        prepare_cpd_features(cpd_folder_name, lookback_window_length)[\n",
    "            [\"ticker\", \"cp_location_norm\", \"cp_score\"]\n",
    "        ]\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"cp_location_norm\": f\"cp_rl_{lookback_window_length}\",\n",
    "                \"cp_score\": f\"cp_score_{lookback_window_length}\"\n",
    "            }\n",
    "        )\n",
    "        .reset_index(),\n",
    "        on =[\"date\", \"ticker\"]\n",
    "    )\n",
    "\n",
    "    features.index = features[\"date\"]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the wrds data with date and price as a dataframe for input to deep_momentum_features()\n",
    "#Then we pass this features df to include_changepoint_features() along with the cpd folder and window length to find this file and add to the features\n",
    "\n",
    "\n",
    "#Then save features to a csv to import to backtest (This will need to be done for each company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "\n",
    "\n",
    "conn = wrds.Connection()\n",
    "\n",
    "ticker = 'GOOGL'\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    d.date,\n",
    "    n.ticker,\n",
    "    d.prc / d.cfacpr as close\n",
    "FROM\n",
    "    crsp.dsf as d\n",
    "JOIN \n",
    "    crsp.dsenames as n on d.permno = n.permno\n",
    "WHERE\n",
    "    n.ticker = '{ticker}' \n",
    "    and date BETWEEN '2016-01-01' and '2023-12-31'\n",
    "ORDER BY\n",
    "    date\n",
    "\"\"\"\n",
    "\n",
    "df = conn.raw_sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "\n",
    "features = deep_momentum_features(df.copy())\n",
    "features = features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/ADM.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/alb_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/alco_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/bg_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/bkng_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/cah_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/cmcsa_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/dis_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/duk_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/fcx_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/fdp_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/googl_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/gww_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/hsic_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/lmnr_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/mck_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/meta_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/nee_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/nem_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/pep_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/rgld_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/scco_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/twx_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/t_lbw21.csv\n",
      "C:/Users/Maxim/Desktop/DDMIF/Changepoint files/vz_lbw21.csv\n"
     ]
    }
   ],
   "source": [
    "features = include_changepoint_features(features, \"C:/Users/Maxim/Desktop/DDMIF/Changepoint files/\", 21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
