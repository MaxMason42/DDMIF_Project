{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MaxMason42/DDMIF_Project.git"
      ],
      "metadata": {
        "id": "VXp65I0reekJ",
        "outputId": "5b229b08-3ee6-42f0-fc5b-19e20dede789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DDMIF_Project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DDMIF_Project/\n"
      ],
      "metadata": {
        "id": "AomXtdvYeloB",
        "outputId": "e30d31e7-fd1b-444d-e300-203329b7e71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DDMIF_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.17.3"
      ],
      "metadata": {
        "id": "53qeqTM5fTap",
        "outputId": "78c6f6a0-7242-4b13-bb75-156fa7cec238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.17.3\n",
            "  Downloading protobuf-3.17.3-py2.py3-none-any.whl.metadata (858 bytes)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from protobuf==3.17.3) (1.16.0)\n",
            "Downloading protobuf-3.17.3-py2.py3-none-any.whl (173 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/173.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m122.9/173.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.9/173.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-api-core 2.19.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-aiplatform 1.73.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-functions 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-iam 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-language 2.15.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-resource-manager 1.13.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "google-cloud-translate 3.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.17.3 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.17.3 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "proto-plus 1.25.0 requires protobuf<6.0.0dev,>=3.19.0, but you have protobuf 3.17.3 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.17.3 which is incompatible.\n",
            "tensorflow-hub 0.16.1 requires protobuf>=3.19.6, but you have protobuf 3.17.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.17.3 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.1 which is incompatible.\n",
            "wandb 0.18.7 requires protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 3.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.17.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "9c0fac8bdf5e4f8fa50572bc41703ed7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fwIB5TcteL4-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from backtest import run_all_windows\n",
        "\n",
        "\n",
        "HP_MINIBATCH_SIZE = [64, 128, 256]\n",
        "TICKERS = ['ADM', 'ALB', 'ALCO', 'AMZN', 'BA', 'BAC', 'BG', 'BKNG', 'C', 'CAH', 'CMCSA', 'DHI', 'DIS', 'DUK', 'FCX', 'FDP',\n",
        "           'GOOGL', 'GPC', 'GS', 'GWW', 'HSIC', 'INTC', 'JPM', 'KO', 'LEN', 'LMNR', 'MCK', 'META', 'NEE', 'NEM', 'NVR',\n",
        "           'PEP', 'PG', 'PHM', 'RGLD', 'SCCO', 'T', 'TOL', 'TWX', 'VZ', 'WFC', 'WMT']\n",
        "MODLE_PARAMS = {\n",
        "    \"architecture\": \"TFT\",\n",
        "    \"total_time_steps\": 252,\n",
        "    \"early_stopping_patience\": 25,\n",
        "    \"multiprocessing_workers\": 32,\n",
        "    \"num_epochs\": 300,\n",
        "    \"early_stopping_patience\": 25,\n",
        "    \"fill_blank_dates\": False,\n",
        "    \"split_tickers_individually\": True,\n",
        "    \"random_search_iterations\": 50 ,\n",
        "    \"evaluate_diversified_val_sharpe\": True,\n",
        "    \"train_valid_ratio\": 0.90,\n",
        "    \"time_features\": False,\n",
        "    \"force_output_sharpe_length\": 0,\n",
        "}\n",
        "\n",
        "\n",
        "TEST_MODE = True\n",
        "ASSET_CLASS_MAPPING = dict(zip(TICKERS, [\"STOCK\"] * len(TICKERS)))\n",
        "TRAIN_VALID_RATIO = 0.9\n",
        "TIME_FEATURES = False\n",
        "FORCE_OUTPUT_SHARPE_LENGTH = None\n",
        "EVALUATE_DIVERSIFIED_VAL_SHARPE = True\n",
        "NAME = \"stock\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBTShDSxeL4_"
      },
      "outputs": [],
      "source": [
        "folder_path = \"Data/Finished_Datasets/\"\n",
        "combined_data = []\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".csv\"):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        data = pd.read_csv(file_path)\n",
        "\n",
        "        combined_data.append(data)\n",
        "\n",
        "\n",
        "combined_df = pd.concat(combined_data, ignore_index=True)\n",
        "\n",
        "combined_df = combined_df.drop(columns=[\"close\", \"srs\"])\n",
        "\n",
        "\n",
        "combined_df.to_csv(\"Data/full_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bmhSl8HheL4_"
      },
      "outputs": [],
      "source": [
        "def run_test(experiment: str, train_start: int, test_start: int, test_end: int, test_window_size: int, num_repeats: int):\n",
        "\n",
        "    if experiment == \"LSTM\":\n",
        "        architecture = \"LSTM\"\n",
        "        lstm_time_steps = 63\n",
        "        changepoint_lbws = [21]\n",
        "    elif experiment == \"TFT\":\n",
        "        architecture = \"TFT\"\n",
        "        lstm_time_steps = 252\n",
        "        changepoint_lbws = [21]\n",
        "    elif experiment == \"TFT-SHORT\":\n",
        "        architecture = \"TFT\"\n",
        "        lstm_time_steps = 63\n",
        "        changepoint_lbws = [21]\n",
        "    else:\n",
        "        raise BaseException(\"Invalid experiment.\")\n",
        "\n",
        "    versions = range(1, 1 + num_repeats) if not TEST_MODE else [1]\n",
        "\n",
        "    experiment_prefix = (\n",
        "        NAME\n",
        "        + (\"_TEST\" if TEST_MODE else \"\")\n",
        "        + (\"\" if TRAIN_VALID_RATIO == 0.90 else f\"_split{int(TRAIN_VALID_RATIO * 100)}\")\n",
        "    )\n",
        "\n",
        "    cp_string = (\n",
        "        \"none\"\n",
        "        if not changepoint_lbws\n",
        "        else reduce(lambda x, y: str(x) + str(y), changepoint_lbws)\n",
        "    )\n",
        "    time_string = \"time\" if TIME_FEATURES else \"notime\"\n",
        "    _project_name = f\"{experiment_prefix}_{architecture.lower()}_cp{cp_string}_len{lstm_time_steps}_{time_string}_{'div' if EVALUATE_DIVERSIFIED_VAL_SHARPE else 'val'}\"\n",
        "\n",
        "    if FORCE_OUTPUT_SHARPE_LENGTH:\n",
        "        _project_name += f\"_outlen{FORCE_OUTPUT_SHARPE_LENGTH}\"\n",
        "    _project_name += \"_v\"\n",
        "\n",
        "    for v in versions:\n",
        "        PROJECT_NAME = _project_name + str(v)\n",
        "\n",
        "        intervals = [\n",
        "            (train_start, y, y + test_window_size)\n",
        "            for y in range(test_start, test_end)\n",
        "        ]\n",
        "\n",
        "\n",
        "        params = MODLE_PARAMS.copy()\n",
        "        params[\"total_time_steps\"] = lstm_time_steps\n",
        "        params[\"architecture\"] = architecture\n",
        "        params[\"evaluate_diversified_val_sharpe\"] = EVALUATE_DIVERSIFIED_VAL_SHARPE\n",
        "        params[\"train_valid_ratio\"] = TRAIN_VALID_RATIO\n",
        "        params[\"time_features\"] = TIME_FEATURES\n",
        "        params[\"force_output_sharpe_length\"] = FORCE_OUTPUT_SHARPE_LENGTH\n",
        "\n",
        "\n",
        "        if TEST_MODE:\n",
        "            params[\"num_epochs\"] = 1\n",
        "            params[\"random_search_iterations\"] = 2\n",
        "\n",
        "        features_file_path = \"Data/full_data.csv\"\n",
        "\n",
        "        run_all_windows(\n",
        "            PROJECT_NAME,\n",
        "            features_file_path,\n",
        "            intervals,\n",
        "            params,\n",
        "            changepoint_lbws,\n",
        "            ASSET_CLASS_MAPPING,\n",
        "            [32, 64, 128] if lstm_time_steps == 252 else HP_MINIBATCH_SIZE,\n",
        "            test_window_size,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rqM9Jyk7eL4_",
        "outputId": "90f2a8a1-f5e3-4565-e797-c41bd04c938d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 00m 08s]\n",
            "sharpe: 0.879293042518289\n",
            "\n",
            "Best sharpe So Far: 0.879293042518289\n",
            "Total elapsed time: 00h 00m 15s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LstmDeepMomentumNetworkModel' object has no attribute 'get_positions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1fa1e97ac092>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2023\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-1a97bd12ebe8>\u001b[0m in \u001b[0;36mrun_test\u001b[0;34m(experiment, train_start, test_start, test_end, test_window_size, num_repeats)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mfeatures_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Data/full_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         run_all_windows(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mPROJECT_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfeatures_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DDMIF_Project/backtest.py\u001b[0m in \u001b[0;36mrun_all_windows\u001b[0;34m(experiment_name, features_file_path, train_intervals, params, changepoint_lbws, asset_class_dictionary, hp_minibatch_size, standard_window_size)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# run the expanding window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minterval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_intervals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         run_single_window(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mfeatures_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DDMIF_Project/backtest.py\u001b[0m in \u001b[0;36mrun_single_window\u001b[0;34m(experiment_name, features_file_path, train_interval, params, changepoint_lbws, skip_if_completed, asset_class_dictionary, hp_minibatch_size)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mmodel_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     )\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdmn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best validation loss = {val_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DDMIF_Project/deep_momentum_network.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, data, model)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_diversified_val_sharpe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LstmDeepMomentumNetworkModel' object has no attribute 'get_positions'"
          ]
        }
      ],
      "source": [
        "run_test(\"LSTM\", 2017, 2019, 2023, 1, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UGhiALJeL4_"
      },
      "outputs": [],
      "source": [
        "raw_data = pd.read_csv(\"Data/full_data.csv\", index_col=0, parse_dates=True)\n",
        "raw_data.rename(columns={'date.1': 'date'}, inplace=True)\n",
        "raw_data[\"date\"] = raw_data[\"date\"].astype(\"datetime64[ns]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQljgNhGeL5A"
      },
      "outputs": [],
      "source": [
        "df = raw_data.dropna()\n",
        "df = df[df[\"year\"] >= 2017].copy()\n",
        "years = df[\"year\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZp9EjnzeL5A",
        "outputId": "299ad18e-3a4c-47ad-bc59-e896e008bd90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7Ase2sZeL5A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = r\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2\"\n",
        "os.environ['PATH'] += r\";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\"\n",
        "os.environ['PATH'] += r\";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/extras/CUPTI/libx64\"\n",
        "os.environ['PATH'] += r\";C:/tools/cuda/bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsS91id6eL5A",
        "outputId": "f2abed55-ece1-440b-dcce-14b46d35f324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memory growth set for GPUs.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for gpu in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"Memory growth set for GPUs.\")\n",
        "    except RuntimeError as e:\n",
        "        print(\"Error setting memory growth:\", e)\n",
        "else: print(\"No GPUs found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovhf0lXKeL5A",
        "outputId": "1780532a-e8e8-41e0-b5c7-f2e0148d1731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuBEIh7WeL5A"
      },
      "outputs": [],
      "source": [
        "test = \"results\\stocks_TEST_lstm_cp21_len63_notime_div_v1\\2019-2020\\hp\\experiment_stocks_TEST_lstm_cp21_len63_notime_div_v1\\trial_411433bdbcbf8165d1cbd9e69e534202\\checkpoints\\epoch_0\\checkpoint_temp/part-00000-of-00001.data-00000-of-00001.tempstate1022316393468577740\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75nB-a2EeL5A",
        "outputId": "e605a47b-1efe-4e3b-d29a-388fe5e3f53c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'results\\\\stocks_TEST_lstm_cp21_len63_notime_div_v1\\x819-2020\\\\hp\\\\experiment_stocks_TEST_lstm_cp21_len63_notime_div_v1\\trial_411433bdbcbf8165d1cbd9e69e534202\\\\checkpoints\\\\epoch_0\\\\checkpoint_temp\\\\part-00000-of-00001.data-00000-of-00001.tempstate1022316393468577740'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.normpath(test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}